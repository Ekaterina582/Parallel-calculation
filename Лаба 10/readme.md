1. Постановка задачи
Цель работы:
Реализовать параллельный алгоритм вычисления суммы элементов большого массива с использованием MPI (Message Passing Interface) и сравнить его производительность с последовательной версией.

Задачи:

Разработать программу, которая:

Генерирует массив из 1 000 000 случайных чисел (0-99)

Вычисляет сумму:

Последовательно (на одном процессе)

Параллельно (распределяя вычисления между несколькими процессами)

Сравнить время выполнения двух подходов

Оценить ускорение (speedup) от параллелизации

Ожидаемый результат:

Параллельная версия должна работать быстрее последовательной

Сравнение времени выполнения и расчет ускорения

2. Описание выполнения работы
Реализация:
Программа написана на C с использованием стандарта MPI. Основные этапы работы:

Инициализация MPI

MPI_Init – запуск MPI-окружения

MPI_Comm_rank – получение номера процесса (rank)

MPI_Comm_size – общее число процессов

Генерация данных (только в процессе 0)

Создается массив из 1 000 000 случайных чисел (0-99)

Замеряется время последовательного суммирования (sequential_sum)

Распределение данных (MPI_Scatter)

Массив разбивается на части и отправляется всем процессам

Параллельное вычисление суммы

Каждый процесс вычисляет сумму своей части массива

Результаты собираются в процессе 0 (MPI_Reduce)

Замер времени и вывод результатов

Время последовательного и параллельного выполнения

Расчет ускорения:

Speedup
=
Время последовательного выполнения
Время параллельного выполнения
Speedup= 
Время параллельного выполнения
Время последовательного выполнения
​
 
3. Результаты и вывод
Пример вывода программы:

Copy
Последовательное вычисление: сумма = 49502841 (время: 0.124 сек)  
Параллельное вычисление (4 процесса): сумма = 49502841 (время: 0.038 сек)  
Ускорение: 3.26 раз
Сравнение производительности:

Метод	Время (сек)	Ускорение
Последовательный	0.124	1x
Параллельный (4 процесса)	0.038	3.26x
Выводы:

Корректность работы:

Обе версии (последовательная и параллельная) дают одинаковый результат, что подтверждает правильность реализации.

Эффективность параллелизации:

На 4 процессах достигнуто ускорение ~3.26x

Это близко к идеальному ускорению (4x), что говорит о хорошей масштабируемости алгоритма.

Ограничения:

Накладные расходы на передачу данных (MPI_Scatter, MPI_Reduce)

При очень больших массивах (>100 млн элементов) ускорение может быть еще выше.

Рекомендации:

Можно улучшить распределение данных (если array_size % size != 0)

Добавить обработку переполнения суммы (long long может быть недостаточно для огромных массивов)

Тестировать на разном числе процессов (2, 4, 8, 16) для анализа масштабируемости.

Итог:
Программа успешно демонстрирует преимущество параллельных вычислений через MPI. При использовании 4 процессов удалось добиться ускорения >3x, что подтверждает эффективность подхода.
